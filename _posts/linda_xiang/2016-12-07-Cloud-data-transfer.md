---
layout: post
title:  "Use Git to Manage the Big Genomic Data Migration in Cloud"
breadcrumb: true
author: linda_xiang
date: 2016-12-06
categories: linda_xiang
tags:
    - Git
    - Cloud
    - Elasticsearch
teaser:
    info: A simple, yet flexible, efficient and intelligent approach to migrate the big genomic data among cloud storage systems.
    image: linda_xiang/data-transfer/data_migrate_block.png # optional
header: 
    version: small
    title: Software Engineering Blog
    image: header-logo-crop.png
    icon: icon-blog
---

## Introduction

In the age of Big Genomic Data, many academic institutions are faced with the challenge of sharing genomic research data to investigators and research scientists worldwide. The clouds offer scalable and competitively priced computing resources for the analysis and storage of data from large-scale genomics studies. They are now offered by both commercial companies including Amazon, Google and Microsoft, as well as some academic data centres such as [Cancer Genome Collaboratory](https://www.cancercollaboratory.org/) at OICR. Provided the cloud infrastructures and computing enviroments are operational, migrating the big genomic data among those cloud storage systems becomes crucial and urgent.

While migrating the tremendous amount of data generated by NGS([Next Generation Sequencing](https://en.wikipedia.org/wiki/DNA_sequencing#Next-generation_methods)) among those clouds, it is more efficient and realistic to use as many virtual machines as possible to maximize bandwidth utilization, so as to do the data migration in paralle. Given the size of each file(e.g, in average 200GB for each whole genome aligned BAM file) and the large number of files(e.g, in total 5789 whole genome aligned BAM files in our last transfer) to be transferred, scheduling and optimization become both complex and crucial. 

Therefore, a real-time management system is needed to synchronize all jobs performed by different machines. We had to come up with a simple, yet flexible, efficient and smart way to manage the entire workflow.


## System design and implementation

Here we propose a new solution combines the extremely popular [Git](https://git-scm.com/) and [Elasticsearch](https://www.elastic.co/), giving you a very flexible approach that will work with data of any size and amount. 

Let's see what compoments we will be working with today:

<figure>
    <img src="{{site.urlimg}}linda_xiang/data-transfer/data_migrate_block.png" />
    <figcaption>Figure 1. Data Migration Management System Blockgram</figcaption>
</figure>

* Source: Where the big genomic data files will be downloaded from.
* Destination: Where the big genomic data files will be uploaded to.
* Elasticsearch Index: Parse the metadata of data source, generate the JSON format metadata documents and build the Elasticsearch index.
* Jobs Generator: Query the elasticsearch to generate jobs in JSON which contain the important metadata information about the files to be transferred.
* Jobs Repo (Git): Where your jobs live. We can use public space like [GitHub](https://github.com/) or self-hosting git repository to store jobs.
* Cloud Instance: Fetches and moves the job from and among folders in Git repo, downloads the file from the given source and uploads the file to the given destination. 


## How it works

Following I will give a concrete example to show you how to migrate PCAWG data from data source([PCAWG in GNOS](http://pancancer.info/)) to data destination([Collaboratory](https://www.cancercollaboratory.org/)). Here is the system workflow.

<figure>
    <img src="{{site.urlimg}}linda_xiang/data-transfer/data_transfer.png" />
    <figcaption>Figure 2. PCAWG Data Migration From GNOS to Collaboratory</figcaption>
</figure>

#### Data Source: [PCAWG in GNOS](http://pancancer.info/)
The Pan-Cancer Analysis of Whole Genomes project(PCAWG) is an effort to understand the role of non-coding regions of the genome in cancer. For this purpose many different datasets were generated and stored in individual GNOS repositories hosted at different academic centres globally. 

For 2834 donors from 48 cancer projects, the PCAWG project generated uniformly aligned (by BWA-MEM) sample-level BAM files, plus sets of germline variant and somatic mutation data (in VCF format) produced by PCAWG core pipelines. Some uniformly aligned RNA-Seq data for matched genomic data are also available. The size of the datasets are very large, including:

* 703.89 TB WGS BWA aligned BAM data
* 695.59 GB WGS Sanger/DKFZ/Broad/Muse variant calling VCF data
* 24.52 TB RNA-Seq TopHat2/Star aligned BAM data 

#### The Coach: Elasticsearch Index
The PCAWG parser takes a list of GNOS metadata XML files as input, one XML per GNOS Analysis Object. It extracts most important information out for each analysis object(ie, a BAM entry), then create its associated donor entry and/or specimen entry if one does not yet exist. These JSON docs are pushed to Elasticsearch for easy search/browse later in job generator.

#### The Engine: Jobs Generator
The job generator will query the Elasticsearch, retrieve and organize the important metadata related to each GNOS analysis object into a single JSON file. E.g.,

~~~~
{
    "aliquot_id": "04db8bef-8777-48ac-bc2e-3c9acb103f48", 
    "available_repos": [
        {
            "https://gtrepo-ebi.annailabs.com/": {
                "file_md5sum": "de3d65673a34470da768ca7d4135ccbd", 
                "file_size": 31867
            }
        }, 
        {
            "https://gtrepo-bsc.annailabs.com/": {
                "file_md5sum": "cf09ea2d42b3e04c7ce39de4f40ac15a", 
                "file_size": 31868
            }
        }
    ], 
    "data_type": "WGS-BWA-Tumor", 
    "files": [
        {
            "file_md5sum": "8a4e304e7b2742d2214aad519ba2f00f", 
            "file_name": "8a4e304e7b2742d2214aad519ba2f00f.bam", 
            "file_size": 111511575598, 
            "object_id": "c255b8ff-267a-5c7c-9262-80647fa064f9"
        }, 
        {
            "file_md5sum": "8e679ea1b673568c7da57c822f8f650c", 
            "file_name": "8a4e304e7b2742d2214aad519ba2f00f.bam.bai", 
            "file_size": 20316224, 
            "object_id": "b9787ed5-6fb6-573f-94ce-4cb36d3e5c07"
        }, 
        {
            "file_md5sum": "de3d65673a34470da768ca7d4135ccbd", 
            "file_name": "001a5fa1-dcc8-43e6-8815-fac34eb8a3c9.xml", 
            "file_size": 31867, 
            "object_id": "fee46097-9d12-5e30-90ba-0b7172a06c25"
        }
    ], 
    "gnos_id": "001a5fa1-dcc8-43e6-8815-fac34eb8a3c9", 
    "gnos_repo": [
        "https://gtrepo-ebi.annailabs.com/"
    ], 
    "is_santa_cruz": false, 
    "project_code": "RECA-EU", 
    "specimen_type": "Primary tumour - solid tissue", 
    "submitter_donor_id": "C0015", 
    "submitter_sample_id": "C0015T", 
    "submitter_specimen_id": "C0015T"
}
~~~~

Obviously, the job generator will work as a engine to tell the worker where to get the data, what is the data about and also request metadata service to get the file specific object_id for tracking purpose.

#### The Manager: Git Jobs Repo
The remote git repository is the place where we will store our jobs and their migration status. The jobs are organized into the following folder structure:

~~~~
├── backlog-jobs
├── completed-jobs
├── downloading-jobs
├── failed-jobs
├── queued-jobs
└── uploading-jobs
~~~~

It should be accessible to all the machines that do the data migration jobs. If you feel comfortable, you can choose to use a public space like [GitHub](https://github.com/) to host those files, but this comes with the risk of accidentally pushing sensitive data to a world-readable location. Therefore, we can instead use self-hosting git repository solution, so that we can install and use on our own machines. 

#### The Worker: Cloud Instance
Following the [user guide](http://docs.openstack.org/user-guide/dashboard-launch-instances.html), it is easy to spin up as many instances as needed in Collaboratory. While many softwares are required to be installed before running the data migration job. E.g, 

* [gtdownload](https://hpc.nih.gov/apps/GeneTorrent.html): tool to download the data from GNOS
* [icgc-storage-client](http://docs.icgc.org/cloud/guide/#storage-client-usage): tool to Upload/Download/View/Mount data to/from/among Collaboratory

Therefore, it should be more convinient to install everything in one instance, create image and spin up instances from that image. 

To automate the job, we have developped a [tool](https://github.com/ICGC-TCGA-PanCancer/s3-data-qc) to integrate the above tools to do the data migration jobs. Once the instance is set up, you can follow the steps to install the tool:

~~~~
# install pipsi
curl https://raw.githubusercontent.com/mitsuhiko/pipsi/master/get-pipsi.py | python

# clone this tool
git clone git@github.com:ICGC-TCGA-PanCancer/s3-data-qc.git

# install this tool
cd s3-data-qc
pipsi install --editable .

# run the tool
s3objectqc
~~~~

#### Data Destination: [Collaboratory](https://www.cancercollaboratory.org/)
The Cancer Genome Collaboratory (a.k.a [Collaboratory](https://www.cancercollaboratory.org/)) is an academic research cloud that contains the raw and interpreted data from the [International Cancer Genome Consortium](https://dcc.icgc.org/), an international project that aims to sequence the genomes of 25,000 tumours and matching normal tissues. The Collaboratory infrastructure hosts an OpenStack cloud with more than 2592 CPU cores and over 4.4 PB of storage.


#### Put All Together: The Team Work

All these players work as a team to provide an end-to-end data migration management solution in cloud.

* Elasticsearch index will be built nightly to pick up completed jobs from the Git repo and marks relevent flags accordingly
* Job generator talk to Elasticsearch and metadata service to generates JSON files in bulk and check into this Git repo under backlog-jobs
* Orchestrator examines the jobs in backlog-jobs, manually moves high priority jobs to queued-jobs
* The lauched cloud instances periodly checked the queued-jobs, once there is a job:
  * Running worker moves the job to downloading-jobs
  * The worker verifies whether GNOS metadata XML has not been altered comparing to when the job was generated. Job is moved to failed-jobs if there is any mismatch. If everything is normal, the real downloading process will be triggered.
  * Once download completes, job is moved to uploading-jobs
  * Once upload completes, job is moved to completed-jobs
  * If an error is detected by the worker, job is moved to failed-jobs
* The process above is repeated until the all Jobs are moved to completed-jobs folder.

It is obvious that all these procedures are completely visible to all the workers and anyone who has the authority to check out the Git repo. It is very convenient and straightforward for the user to monitor the whole workflow. Since all historical actions are automatically recorded as commits by Git, in-depth reports on the system performance can be easily generated by using data mining techniques.

## Conclusion

The presented solution is the result of work that we put into building the system to migrate huge-scale PCAWG data to Collaboratory/Amazon S3 cloud storage. We needed a simple, yet flexible, efficient and real-time management system. Combining Git, Github, Elasticsearch and Cloud instances provided a very satisfactory solution that we encourage you to give it a try.


